\section{Methods}
\subsection{System Overview}

We propose a \textbf{two-agent hierarchical framework} for epidemic forecasting that explicitly separates \emph{context interpretation} from \emph{probabilistic prediction}. 
Unlike classical compartmental models (e.g., SEIR) or purely data-driven time-series models, the proposed system treats the epidemic time series and the surrounding contextual information as two distinct but coupled channels. 
The goal is to forecast weekly HFMD (Hand, Foot and Mouth Disease) incidence while explicitly conditioning on exogenous drivers such as school calendars, weather, and higher-level surveillance signals.

The framework consists of two specialized agents:

\begin{itemize}
    \item \textbf{Agent 1 (Event Interpreter)}: A large language model (LLM) that ingests heterogeneous external data---including weather summaries, school calendars, and aggregated regional statistics---together with retrieval-augmented domain knowledge (e.g., guidelines, policy documents). It produces a scalar \emph{transmission impact signal} that summarizes how current conditions are expected to affect HFMD transmission in the near future.
    \item \textbf{Agent 2 (Forecast Generator)}: A probabilistic forecasting module that combines the recent epidemic trajectory with the transmission impact signal from Agent~1 to generate distributional forecasts of weekly HFMD counts, calibrated via Poisson/Negative Binomial likelihoods.
\end{itemize}

This role separation yields a modular architecture in which each agent can be independently replaced or improved. 
For example, the LLM backbone in Agent~1 or the time-series backbone in Agent~2 (e.g., foundation model vs.\ parametric baseline) can be swapped without changing the overall interface between the two agents.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/figure1.png}
    \caption{Overall hierarchical neuro-symbolic architecture for HFMD forecasting. 
    Agent~1 interprets heterogeneous contextual signals (school calendar, weather, national HFMD trends, guidelines) and emits a scalar transmission impact signal, while Agent~2 combines this signal with historical case counts to produce probabilistic forecasts.}
    \label{fig:hierarchical-architecture}
\end{figure}


\subsection{Agent 1: Context-Aware Event Interpretation}

\subsubsection{Input data collection and preprocessing}

The primary role of Agent~1 is to aggregate diverse external signals and convert them into a unified scalar index, the \textbf{Transmission Impact Score} $I_t \in [-1.0, 1.0]$, together with a concise natural-language summary of the situation at week $t$. 
In this work, Agent~1 draws on three main categories of inputs.

\paragraph{Weather data}

Daily meteorological observations are collected from local weather stations, including mean temperature (°C), relative humidity (\%), and total precipitation (mm). 
These daily records are aggregated to a weekly resolution to match the HFMD surveillance series. 
For each epidemiological week $t$, we compute 7-day summaries such as average temperature, average humidity, and cumulative precipitation, and encode them as a compact JSON object that is passed to Agent~1 as part of its structured input.

\paragraph{Social events and school calendar}

Social events are derived from a pre-defined event calendar. 
Each epidemiological week is labeled with the corresponding school status (e.g., \texttt{in\_session}, \texttt{summer\_break}, \texttt{winter\_break}) and public holidays (e.g., Spring Festival, National Day). 
These variables are represented as discrete categorical features in the JSON input.
For example, the week containing 1 February 2024 is labeled as \texttt{winter\_break + Spring~Festival}, whereas the week containing 19 February 2024 is labeled as \texttt{in\_session}. 
Agent~1 is instructed to treat school status as a primary driver of HFMD transmission, consistent with the pediatric nature of the disease.

\paragraph{Higher-level government surveillance reports}

Government surveillance signals are extracted from official epidemiological bulletins issued by the Chinese Center for Disease Control and Prevention (China CDC) and the Zhejiang provincial health authorities. 
These bulletins provide monthly HFMD case counts at the city and provincial levels. 
For each target week $t$, we construct summary statistics over the preceding one to two months, such as total monthly HFMD counts and month-over-month growth rates at the prefecture/province level. 
These statistics are serialized into JSON and supplied to the LLM so that Agent~1 can reason about the local hospital series in the broader regional context.

% Later subsections (not shown here) describe how Agent~1 maps these inputs
% to (i) a scalar transmission impact score I_t and confidence, and
% (ii) textual explanations (event_summary, risk_notes, lag_rationale),
% which are then consumed by Agent~2.


\subsubsection{Retrieval-Augmented Generation Strategy}

Large language models often struggle to consistently utilize long or heterogeneous contextual inputs when they are provided in a single prompt. 
To improve stability and ensure that the model grounds its reasoning in established epidemiological knowledge, we incorporate a \textbf{retrieval-augmented generation (RAG)} mechanism.

In the knowledge-base construction stage, official HFMD diagnosis and management guidelines published by the China CDC are collected and segmented into short text chunks (maximum 500 characters). 
Each chunk is embedded using the \texttt{all-MiniLM-L6-v2} encoder and stored in a FAISS vector index.

In the dynamic query construction stage, retrieval queries are composed adaptively based on contextual signals for the current week. 
For seasonal factors, the query includes terms such as “HFMD peak season spring summer” during May–July, or “winter low transmission” during January–February. 
For environmental factors, the query is expanded with “weather temperature impact transmission” when meteorological data are present. 
For social factors, we append “school in\_session outbreak children” when schools are in session.

In the retrieval and prompt-construction stage, the composed query is used to retrieve the top-$k$ ($k=2$) most relevant guideline passages (up to 1200 characters). 
These retrieved passages are inserted at the beginning of the LLM system prompt, ensuring that the model’s reasoning is anchored to authoritative, domain-specific evidence rather than relying solely on long free-form contextual descriptions.


\subsubsection{Trend Analysis and Statistical Feature Computation}

To help Agent~1 reason about the recent epidemic trajectory, we compute several simple but informative statistics from the past few weeks of HFMD activity.

\paragraph{Growth Rate.} 
We estimate whether the series is accelerating or decelerating by measuring its relative change over the previous four weeks:
\[
g_t = \frac{y_t - y_{t-4}}{\max(1.0, y_{t-4})}.
\]
The denominator is lower-bounded by 1.0 to avoid instability when past counts are close to zero.

\paragraph{Consecutive Growth.} 
Short-term momentum is captured using the length of the most recent monotonic increase streak:
\[
w_{\text{grow}} = \max\{k \mid y_{t-i+1} > y_{t-i}, \forall i \in [1, k]\}.
\]
A longer streak indicates sustained upward pressure rather than a one-off fluctuation.

\paragraph{Peak Status.} 
We also assess whether the current level is unusually high relative to historical activity. 
If the current observation exceeds the 90th percentile of the full series, it is labeled as a peak:
\[
\text{is\_at\_peak} =
\begin{cases}
1, & \text{if } y_t \geq P_{90}(Y_{\text{full}}), \\
0, & \text{otherwise},
\end{cases}
\]
where $P_{90}(Y_{\text{full}})$ denotes the empirical 90th percentile. 
This feature helps distinguish routine seasonal variation from rare outbreak-level surges.


\subsubsection{LLM-Based Interpretation and Outputs}

Agent 1 passes the above information to the LLM to generate four main outputs.  
First, the transmission impact score $I_t \in [-1.0, 1.0]$ reflects the net effect of external factors on transmission. Values $I_t > 0$ indicate promotion of transmission (e.g., warm weather plus school in session), $I_t < 0$ indicate suppression (e.g., winter plus vacation), and $I_t \approx 0$ indicates a neutral context.

Second, the confidence score $C_t \in [0.0, 1.0]$ represents the model’s confidence in the data quality and signal consistency. For example, missing meteorological data or conflicting signals lead to lower confidence.

Third, an event summary provides a natural-language interpretation of the current situation (e.g., “School reopening coincides with warm weather”).

Fourth, risk notes list key risk factors (e.g., “Rapid growth observed”, “Temperature in optimal range”).

The LLM prompt includes the retrieved epidemiological guidelines, recent case trends, and external data, and requests a structured JSON output. The temperature is set to 0.6 to allow moderate creativity while limiting hallucinations.

\subsection{Agent 2: Probabilistic Forecast Generation}

\subsubsection{Modeling Historical Volatility}

Agent 2 begins by estimating the \textbf{inherent volatility} $v$ of the time series, which captures the noise level intrinsic to the data, independent of external factors.

Volatility is computed as the median of recent weekly relative changes over the last eight weeks, with its range clipped to avoid extremes:
\[
v = \text{Clamp}\left(
\text{Median}\left(
\left|
\frac{y_t - y_{t-1}}{\max(1.0, y_{t-1})}
\right|
\right)_{t \in \text{recent}},
\; 0.05,\; 0.50
\right).
\]

This design has two motivations. First, using the median rather than the mean reduces sensitivity to outliers. Second, if volatility falls below 5\%, the model may become overconfident; if it exceeds 50\%, the forecast becomes practically uninformative. Hence we constrain it within a reasonable range.

\subsubsection{LLM-Based Trajectory Forecasting}

Agent 2 takes four types of inputs: the full time series $Y_{\text{full}} = [y_1, y_2, \ldots, y_t]$, the estimated volatility $v$, Agent 1’s outputs ($I_t$, $C_t$, risk notes), and the forecast horizon $h$ (typically eight weeks).

The LLM receives three principal instructions via the system prompt. First, start from the recent trend (momentum) in the data. Second, adjust the trajectory direction according to the transmission impact score $I_t$ (e.g., upward adjustment when $I_t > 0.3$). Third, increase uncertainty as the forecast horizon lengthens or when signals conflict.

In this stage, we explicitly employ an \textbf{inference-time reasoning} mechanism. Internally, the LLM performs chain-of-thought (CoT) reasoning: when the data momentum and the $I_t$ signal conflict (e.g., the data exhibit a sharp increase, but $I_t$ suggests a decline), the model engages in a conflict-resolution process that either prioritizes data momentum or reevaluates the reliability of external signals based on $C_t$. This conflict resolution goes beyond simple weighted averaging and enables context-aware decision-making.

For each forecasted time step $t+k$, the LLM outputs two quantities: 
the median forecast $\hat{y}_{t+k}$ and an uncertainty score $u_{t+k} \in [0,1]$. 
To ensure numerical stability and reproducibility, we apply a strict 
output-validation step. If the model fails to return a well-formed JSON 
object containing valid numerical fields (e.g., non-numeric or missing values), 
the query is reissued once with identical inputs. If the second attempt 
also fails, the corresponding forecast origin is marked as invalid and 
excluded from evaluation. In practice, across all experiments, no forecast 
origins were discarded.

\subsubsection{Distributional Calibration and Quantile Forecasts}

The final forecast includes a 90\% prediction interval (PI). Because infectious disease counts are discrete, non-negative, and often overdispersed, we adopt the \textbf{negative binomial distribution}.

To map the LLM outputs $(\hat{y}_{t+k}, u_{t+k})$ to the parameters of the negative binomial distribution, we use a \textbf{moment matching} approach. Given target mean $\mu = \hat{y}_{t+k}$ and target variance
\[
\sigma^2 = \bigl(\hat{y}_{t+k} \cdot v \cdot (1 + u_{t+k})\bigr)^2,
\]
the parameters of $\text{NB}(n, p)$ are derived as:
\[
n = \frac{\mu^2}{\sigma^2 - \mu}, \quad \text{for } \sigma^2 > \mu,
\]
\[
p = \frac{n}{n + \mu},
\]
where $n$ is the dispersion parameter. When $\sigma^2 \leq \mu$ (underdispersion), we fall back to a Poisson distribution.

The 90\% prediction interval is then obtained from the inverse cumulative distribution function (inverse CDF) of the negative binomial:
\[
Q_{05}(t+k) = F^{-1}_{\text{NB}}(0.05; n, p),
\]
\[
Q_{95}(t+k) = F^{-1}_{\text{NB}}(0.95; n, p).
\]

This formulation prevents negative forecasts at low counts and naturally captures the asymmetric shape of real epidemic incidence distributions.

Through adaptive uncertainty scaling
\[
\sigma^2 = (\mu \cdot v \cdot (1 + u_k))^2,
\]
when the LLM detects a high-risk situation (e.g., $u_k = 0.5$), the variance increases by a factor of $(1.5)^2 = 2.25$. This produces a heavier-tailed negative binomial distribution that better reflects the increased chance of extreme incidence.

\paragraph{Intuitive Interpretation.}  
When Agent 1 outputs a high risk ($I_t > 0.5$) and Agent 2 outputs high uncertainty ($u_{t+k} > 0.7$), the forecast median rises and the prediction interval widens asymmetrically. This corresponds to the statement: “The situation is likely to worsen, but the exact magnitude is uncertain, with particularly elevated upside risk.”

\subsection{Data Sources and Structure}
\label{subsec:data_sources}

\paragraph{Lishui hospital dataset (clinical HFMD visits, 2023--2024).}
The original raw clinical dataset consists of weekly counts of HFMD outpatient visits from a general hospital in Lishui, Zhejiang Province, China, covering the period from January~6, 2020 to September~30, 2024.
Because the COVID-19 pandemic introduced substantial distortions---including lockdown-related care avoidance, abrupt under-reporting, and potential post-lockdown ``immunity rebound''---we restricted model development to a post-stabilization window.
Specifically, we used data from January~9, 2023 through January~29, 2024 for model training, and evaluated 1-week-ahead forecasts on the out-of-sample period from February~1, 2024 through September~30, 2024 (33 epidemiological weeks in total).
Within this 33-week evaluation window, the weekly mean HFMD case count is 5.3, the standard deviation is 4.8, and the maximum is 19 cases, indicating low overall incidence but pronounced seasonal variability with multiple peaks in spring and autumn.

\paragraph{Hong Kong government dataset (population-level surveillance, 2023--2024).}
The second dataset is obtained from the Centre for Health Protection (CHP), Department of Health, Hong Kong SAR, which reports the weekly number of hospital admission episodes associated with HFMD in the public Hospital Authority system.
The full historical time series spans from 2010 through 2025.
For comparability with the Lishui setting and with other baselines, we focus on the period from January~1, 2023 to September~30, 2024, yielding a 90-week evaluation window.
During this 90-week period, the weekly mean HFMD hospitalization count is 8.7, with a standard deviation of 6.2 and a maximum of 31 cases.
Compared with the Lishui clinical dataset, this government surveillance series exhibits a relatively stable pattern with well-defined summer peaks and smoother inter-annual variation.

\subsection{Data Availability and Ethical Considerations}
\label{subsec:data_ethics}

The Hong Kong HFMD surveillance data used in this study are publicly available from the 
Centre for Health Protection (CHP), Department of Health, Hong Kong SAR, as part of their 
routine infectious-disease reporting system.

The Lishui hospital dataset consists of fully de-identified weekly aggregates of HFMD outpatient 
visits extracted from the hospital information system. All records were anonymized by the 
hospital's data management team before being provided to the authors under an institutional 
data-sharing agreement. No individual-level identifiers (such as names, addresses, exact dates 
of birth, or medical record numbers) were accessed or stored at any point, and re-identification 
was not possible.

According to institutional and national regulations, secondary analysis of anonymized, 
aggregate data does not constitute human-subject research and is therefore exempt from 
institutional review board (IRB) approval. No informed consent was required because no 
personally identifiable information was collected or analyzed.


\subsection{Algorithmic Structure and Execution Protocol}

\subsubsection{Rolling Forecast Pipeline}

We implement a rolling forecasting procedure that repeatedly advances the forecast origin along the time series. For each chosen evaluation date, the model uses only information available up to that date and generates a one-step-ahead forecast. Although the architecture can in principle produce multi-step forecasts for horizons $h>1$, all experiments in this study focus on $h=1$ for fair comparison with classical and foundation time-series baselines.

Concretely, we first initialize both agents (Event Interpreter and Forecast Generator) and load the full time series $Y = [y_1, y_2, \dots, y_n]$ together with the corresponding calendar dates. We then define a set of forecast origins (step dates) between a given start date and end date, optionally subsampling if the series is long.

For each forecast origin $t_i$ in this set, we extract the training subset $Y_{\text{train}} = Y[1 : t_i]$ and the recent window of the last eight weeks $Y_{\text{recent}}$. Next, we collect external data up to $t_i$ and for the subsequent horizon $h$ (set to $h=1$ in our experiments), including weather, calendar events, and government statistics.


Agent~1 (Event Interpreter) is then invoked with the disease name, the current date $t_i$, recent history, external data, and the full training history. It returns a structured interpretation consisting of the impact score $I_t$, confidence score $C_t$, and natural-language risk notes.

Agent~2 (Forecast Generator) takes $Y_{\text{recent}}$, the pair $(I_t, C_t)$, the risk notes, the forecast horizon $h$, and the training history as inputs and produces a probabilistic forecast trajectory. For each future week, it outputs the median prediction together with lower and upper quantiles (e.g., 5th and 95th percentiles).

Finally, the forecast is compared against the observed future values $Y[t_i+1 : t_i+h]$ to compute evaluation metrics such as MAE, RMSE, CRPS, coverage, and sharpness. This process is repeated for all forecast origins, and the metrics are aggregated across steps to obtain overall performance.

\subsubsection{External Data Collection}

To support context-aware interpretation, we construct an evidence pack for each forecast origin by aggregating multiple external data sources.

First, for HFMD, we prioritize disease-specific local data. We load weather records (e.g., temperature and precipitation) for the eight weeks preceding the forecast date, government statistics (such as monthly case summaries) for up to six months back, and event information (school status and public holidays) for the horizon weeks ahead. These elements are stored in a dictionary-like structure that is passed to Agent~1.

Second, if a dedicated government monthly report CSV file is available, we parse it up to the current date and merge any additional relevant statistics into the evidence pack. This step allows the model to incorporate aggregated surveillance trends or warning levels issued by public health authorities.

Third, if web-based signals are enabled, we optionally scrape or query web sources (e.g., official websites or news feeds) for the target disease and region as of the forecast date, and integrate these signals into the evidence pack. This can capture emerging events that are not yet reflected in structured datasets.

Finally, if no weather information is present in the evidence pack (for example, if local CSV files are missing), we fall back to querying the Meteostat API to retrieve recent meteorological data for the corresponding location and period. The resulting evidence pack, containing weather, government statistics, and event information, is then supplied to Agent~1 to ground its interpretation in concrete, time-aligned contextual data.


\subsubsection{Model Configuration}

This study employs three advanced large language models as the backbone of Agent~1 and Agent~2: 
\textbf{Qwen3-235B-22A} (Alibaba), \textbf{DeepSeek-Reasoner} (DeepSeek AI), and \textbf{GPT-5.1} (OpenAI). 
These models were selected based on their strong performance in recent reasoning benchmarks and their demonstrated stability in multistep inference tasks, which is essential for the multi-agent pipeline.

All models are used with their official inference settings recommended by each provider. 
Hyperparameters are aligned across both agents unless otherwise noted. 
For temperature, we follow provider-specific optimal values: 
\textbf{0.6 for Qwen3}, \textbf{1.0 for DeepSeek-Reasoner}, and \textbf{0.7 for GPT-5.1}. 
The temperature is kept moderate to preserve reasoning consistency while allowing controlled variability in contextual interpretation.

The maximum generation length is set to 2000 tokens for all models. 
Where available, the highest-level reasoning or “thinking” modes are enabled 
(\texttt{Qwen-deep}, \texttt{DeepSeek-Reasoner}, and \texttt{GPT-high}), 
as these modes improve chain-of-thought quality without requiring fine-tuning. 
Across all providers, the context window is configured to the maximum allowed by the model (up to 128k tokens depending on the platform), ensuring that all historical summaries and retrieved guideline passages fit within a single inference cycle.


\subsubsection{Algorithmic Constants}

\begin{table}[t]
\centering
\caption{System parameters.}
\label{tab:system_params}
\begin{tabular}{l l l}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
horizon & 1 weeks & Forecast window length \\
recent\_window & 8 weeks & Lookback window for trend computation \\
volatility\_min & 0.05 & Minimum allowed volatility \\
volatility\_max & 0.50 & Maximum allowed volatility \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Framework}

Forecast performance is evaluated using three categories of metrics.

\paragraph{Point Forecast Accuracy.}  
We use two metrics: mean absolute error (MAE) and root mean squared error (RMSE):
\[
\text{MAE} = \frac{1}{N}\sum_{i=1}^{N} |y_i - \hat{y}_i|,\quad
\text{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}.
\]

\paragraph{Probabilistic Forecast Quality.}  
We use three metrics. The continuous ranked probability score (CRPS) measures the distance between the predictive distribution and the observed value. Coverage is the proportion of times the 90\% prediction interval contains the true value. Sharpness is the mean width of the prediction interval.

\paragraph{Peak Detection Performance.}  
We use two metrics. Peak recall denotes the proportion of true peak weeks correctly identified, and peak MAE measures the error at peak time points.

We perform rolling forecasts on the Hangzhou hospital dataset (33 steps) and the Hong Kong government dataset (70 steps). For each forecast date, we evaluate 1-step-ahead predictions, resulting in a total of 103 forecast instances for performance assessment.

\section{Experiments}
\label{sec:experiments}

\subsection{Research Questions}

We design our experiments to answer the following research questions:

\begin{itemize}
    \item \textbf{RQ1:} Can LLM-based agents effectively leverage external contextual information (weather, government reports, school calendars) to improve infectious disease forecasting compared to traditional statistical and deep learning methods?
    \item \textbf{RQ2:} How do different LLM architectures (reasoning-focused vs.\ general-purpose) perform in epidemiological forecasting tasks?
    \item \textbf{RQ3:} What is the relative contribution of each system component (contextual interpretation agent, RAG-based guideline retrieval, probabilistic calibration) to overall forecasting performance?
    \item \textbf{RQ4:} Can the proposed framework generalize across different geographical regions and time periods with varying data characteristics?
\end{itemize}

\subsection{Experimental Setup}

\subsubsection{Datasets}

We evaluate the proposed framework on two real-world HFMD surveillance datasets with distinct temporal characteristics.

\paragraph{Hangzhou dataset (2024).}
This dataset covers HFMD cases in Hangzhou, Zhejiang Province, China, from February~1, 2024 to September~30, 2024. It consists of 33 weekly time steps (one-week-ahead forecasting setting) and corresponds to post-COVID surveillance conditions with moderate case counts. The data are obtained from a local hospital surveillance system (\texttt{weekly\_lis}).

\paragraph{Hong Kong dataset (2023--2024).}
This dataset covers HFMD incidence in the Hong Kong Special Administrative Region from January~1, 2023 to September~30, 2024, with more than 90 weekly time steps. It reflects a longer time series with a more established pre-COVID baseline and clear seasonal structure. The data come from public health surveillance reports.

For both datasets we consider:

\begin{itemize}
    \item weekly reported HFMD cases (target variable),
    \item historical case data (2010--2024) for contextual comparison,
    \item weather data (temperature, precipitation),
    \item government epidemiological reports (national and regional),
    \item school calendar information (term dates and public holidays).
\end{itemize}

The weekly case counts are used as univariate forecasting targets; external signals are only used by the LLM-based agents as described in Section~\ref{sec:methods}.

\subsubsection{Baselines}

We compare our two-agent framework against three groups of baselines.

\paragraph{Traditional statistical and epidemiological models.}
\begin{itemize}
    \item \textbf{ARIMA:} Seasonal autoregressive integrated moving-average model.
    \item \textbf{Prophet:} Additive time-series model with seasonality and holiday effects.
    \item \textbf{SEIR:} Compartmental susceptible--exposed--infected--recovered model calibrated to weekly incidence.
\end{itemize}

\paragraph{Machine learning and time-series foundation models.}
\begin{itemize}
    \item \textbf{LSTM:} Long short-term memory network trained on sliding windows.
    \item \textbf{XGBoost:} Gradient-boosted decision trees with hand-crafted lag and seasonal features.
    \item \textbf{TimesFM:} Google's pre-trained time-series foundation model.
    \item \textbf{Chronos:} Amazon's pre-trained probabilistic time-series model.
    \item \textbf{Moirai:} Salesforce's universal time-series forecasting model.
\end{itemize}

\paragraph{LLM-based forecasters without explicit context.}
To isolate the effect of explicit context modeling, we also consider strong LLM baselines that receive only the numeric time series and are prompted to output future case counts (and, if applicable, confidence intervals), without external context, retrieval, or calibration:
\begin{itemize}
    \item \textbf{Qwen3 (no context):} reasoning-capable Chinese--English LLM.
    \item \textbf{GPT-5.1 (no context):} general-purpose OpenAI model.
    \item \textbf{Gemini 3 Pro (no context):} Google's reasoning-focused LLM.
    \item \textbf{DeepSeek-V3 (no context):} DeepSeek's chat-oriented LLM.
\end{itemize}

\paragraph{Proposed two-agent framework.}
Our full method (\textbf{Two-Agent, ours}) uses:
\begin{itemize}
    \item \textbf{Agent~1 (Context Interpreter):} a Qwen3-235B-Deep style reasoning LLM, equipped with RAG over epidemic prevention guidelines and external signals (weather, government reports, school calendars).
    \item \textbf{Agent~2 (Forecast Generator):} the same backbone LLM used to produce median forecasts and uncertainty scores, which are then converted into a negative-binomial predictive distribution via moment matching as described in Section~3.3.
\end{itemize}

\subsubsection{Implementation Details}

For classical baselines (ARIMA, Prophet, SEIR) we use standard implementations in \texttt{statsmodels} and \texttt{prophet}, tuning hyperparameters via grid search on a rolling validation window. LSTM and XGBoost models are implemented in PyTorch and \texttt{xgboost}, respectively, using early stopping based on one-step-ahead MAE. Time-series foundation models (TimesFM, Chronos, Moirai) are used with official or reference implementations and minimal configuration tuning (e.g., scaling, forecast horizon), to avoid overfitting on the relatively short series.

For the two-agent framework, the LLM configurations follow Section~3.6: Agent~1 uses a temperature of 0.6 to allow moderate variability in natural-language reasoning, while Agent~2 uses a temperature of 0.2 for stable numeric predictions. We set the recent window length to 8 weeks and the forecast horizon to 1 week for the main evaluation; extended horizons are considered in supplementary analyses. Negative-binomial calibration via moment matching is applied on the recent 8-week window to obtain 90\% prediction intervals.

All LLM calls are executed via provider APIs (e.g., DashScope, OpenAI, Google, DeepSeek) on commodity cloud machines. Statistical and neural baselines run on a single CPU or a single NVIDIA A100 GPU; the overall computational cost is dominated by LLM inference.

\subsubsection{Evaluation Protocol}

We adopt a rolling-origin evaluation protocol. For each dataset, we select a set of forecast origins in the latter part of the series (to ensure sufficient history), and at each origin $t$ we:

\begin{enumerate}
    \item fit or update each baseline using data up to $t$,
    \item run the two-agent framework using the same historical window and external data available as of $t$,
    \item generate a one-step-ahead forecast $y_{t+1}$ (and, for probabilistic models, a predictive distribution).
\end{enumerate}

We aggregate metrics across all forecast origins. Point forecast accuracy is assessed using mean absolute error (MAE) and root mean squared error (RMSE). Probabilistic accuracy is measured with continuous ranked probability score (CRPS), 90\% interval coverage (fraction of observations falling inside the 90\% prediction interval), and sharpness (average width of this interval). Peak detection performance is further evaluated using peak recall and peak MAE, where peaks are defined as local maxima above the 80th or 90th percentile of the series. For brevity, we focus on MAE, RMSE, and CRPS in the main text and report additional metrics in the appendix.

We also conduct a targeted analysis of ``epidemiologically significant'' periods, including:
\begin{itemize}
    \item \textbf{peak weeks}: weeks whose case counts fall in the top 20\% of the empirical distribution;
    \item \textbf{event periods}: weeks where cases increase by at least a factor of 2 compared to the previous week.
\end{itemize}

\subsection{Main Quantitative Results}

\subsubsection{Overall one-step-ahead performance}

Table~\ref{tab:hangzhou_main} reports the one-step-ahead forecasting performance on the Hangzhou dataset. Our Qwen3-based two-agent framework achieves MAE comparable to, or slightly better than, the best time-series foundation model (TimesFM), and clearly outperforms classical baselines.

\begin{table}[t]
    \centering
    \caption{One-step-ahead forecasting performance on the Hangzhou HFMD dataset (33 weeks). Lower is better for MAE, RMSE, and CRPS.}
    \label{tab:hangzhou_main}
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Method} & \textbf{MAE} & \textbf{RMSE} & \textbf{CRPS} & \textbf{Category} \\
    \midrule
    Two-Agent (Qwen3-Deep, ours)       & \textbf{3.970} & \textbf{6.394} & \textbf{3.970} & LLM-Agent \\
    GPT-4.5-Preview + Agent            & 4.818          & 8.465          & 4.818          & LLM-Agent \\
    DeepSeek-Reasoner + Agent          & 4.077          & 6.179          & 4.077          & LLM-Agent \\
    \midrule
    TimesFM (pretrained)               & 3.972          & 6.472          & 3.972          & TS Foundation \\
    Chronos (pretrained)               & 4.059          & --             & 4.059          & TS Foundation \\
    Moirai (pretrained)                & 4.619          & --             & 4.131          & TS Foundation \\
    \midrule
    ARIMA                              & 4.853          & 7.374          & 4.853          & Statistical \\
    Prophet                            & 4.676          & --             & 4.676          & Statistical \\
    SEIR                               & 4.353          & 6.472          & 4.353          & Epidemiological \\
    LSTM                               & 4.536          & 6.880          & 4.536          & Deep Learning \\
    XGBoost                            & 4.353          & 6.472          & 4.353          & Deep Learning \\
    \bottomrule
    \end{tabular}
\end{table}

On Hangzhou, the Two-Agent (Qwen3-Deep) system attains MAE~3.970, essentially matching TimesFM (3.972) and outperforming all classical baselines. Reasoning-oriented LLM agents (Qwen3-Deep, DeepSeek-Reasoner) tend to perform better than generic LLM agents, suggesting that advanced reasoning capabilities are beneficial even in relatively short epidemiological series.

Table~\ref{tab:hongkong_main} shows the corresponding results on the Hong Kong dataset. Here, the two-agent framework with Qwen as backbone attains the best overall performance, with clear gains over context-free LLM baselines.

\begin{table}[t]
    \centering
    \caption{One-step-ahead forecasting performance on the Hong Kong HFMD dataset (2023--2024). Lower is better for MAE, RMSE, and CRPS.}
    \label{tab:hongkong_main}
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Method} & \textbf{MAE} & \textbf{RMSE} & \textbf{CRPS} & \textbf{Category} \\
    \midrule
    Two-Agent (Qwen, ours)             & \textbf{3.596} & \textbf{5.304} & \textbf{3.596} & LLM-Agent \\
    OpenAI GPT-4o + Agent              & 4.644          & 6.591          & 4.644          & LLM-Agent \\
    Gemini-1.5-Pro + Agent             & 4.644          & 6.591          & 4.644          & LLM-Agent \\
    DeepSeek-V3 + Agent                & 3.596          & 5.304          & 3.596          & LLM-Agent \\
    \midrule
    Qwen (no context)                  & 3.958          & 5.574          & 3.958          & LLM (no context) \\
    OpenAI (no context)                & 4.644          & 6.591          & 4.644          & LLM (no context) \\
    Gemini (no context)                & 4.644          & 6.591          & 4.644          & LLM (no context) \\
    DeepSeek (no context)              & 3.692          & 5.171          & 3.692          & LLM (no context) \\
    \bottomrule
    \end{tabular}
\end{table}

On Hong Kong, adding context interpretation (Agent~1), RAG, and probabilistic calibration improves Qwen's MAE from 3.958 (no context) to 3.596, corresponding to an approximate 9.1\% reduction in absolute error. Similar trends are observed for other backbones, indicating that explicit context modeling is consistently beneficial.

\subsection{Ablation Studies}

To understand the contribution of each component, we perform an ablation study on the Hong Kong dataset. Results are summarized in Table~\ref{tab:ablation}.

\begin{table}[t]
    \centering
    \caption{Ablation study on the Hong Kong dataset. ``Full system'' denotes the complete two-agent architecture with context interpretation, RAG-based guidelines, and negative-binomial calibration.}
    \label{tab:ablation}
    \begin{tabular}{lccc}
    \toprule
    \textbf{Configuration} & \textbf{MAE} & \textbf{RMSE} & $\Delta$\,\textbf{MAE vs.\ full} \\
    \midrule
    Full system (Agent~1 + RAG + NB)   & \textbf{3.596} & \textbf{5.304} & baseline \\
    Without Agent~1 (no context)       & 3.958          & 5.574          & +10.1\% \\
    Without RAG (no guidelines)        & 3.692          & 5.171          & +2.7\% \\
    Without NB calibration             & 3.650          & 5.320          & +1.5\% \\
    Pure LLM on history (no components)& 3.958          & 5.574          & +10.1\% \\
    \bottomrule
    \end{tabular}
\end{table}

Removing Agent~1 (no context interpretation) causes the system to revert to the context-free LLM baseline, increasing MAE by approximately 10.1\%. This indicates that contextual interpretation of weather, reports, and school calendars is the single most influential component.

Disabling RAG---i.e., depriving Agent~1 of guideline retrieval and forcing it to rely only on raw external signals---yields a smaller but consistent degradation of about 2.7\% in MAE, particularly around unusual holiday or weather patterns. Finally, turning off the negative-binomial calibration increases MAE by around 1.5\% and degrades interval reliability, especially at low counts, but leaves point forecasts largely unchanged.

Overall, the components act synergistically: RAG-guided domain grounding improves the quality of Agent~1's impact scores, which in turn guides Agent~2's calibrated forecasts.

\subsection{Qualitative Analysis}

\subsubsection{Case study: spring peak onset in Hangzhou}

We inspect the behavior of the two-agent system around the onset of a spring peak in Hangzhou. In the week of May~6, 2024, schools are in session, temperatures are rising into an average of approximately $22^\circ$C, and provincial HFMD reports indicate increasing activity. Observed cases increase from 13 to 15, marking the beginning of a peak period.

At this forecast origin, the proposed framework predicts a higher trajectory than classical baselines: the two-agent system outputs a median forecast of about 10 cases (MAE $\approx 5$), whereas LSTM and Prophet predict 8 and 6 cases, respectively (MAE $\approx 7$ and $9$). Agent~1 assigns a positive transmission impact score ($I_t \approx +0.45$) and produces a natural-language summary emphasizing three risk factors: (i) schools in session post-holiday, (ii) temperatures entering the optimal range for HFMD transmission (20--25$^\circ$C), and (iii) rising provincial reports with historically typical May peaks. The resulting predictive distribution features an elevated median and an asymmetric upper tail, reflecting increased upward risk. In contrast, baselines that depend solely on recent counts under-react to the regime shift and underestimate the onset of the peak.

\subsubsection{Case study: winter trough in Hangzhou}

We also examine a winter trough period in early February~2024. In the week of February~5, 2024, temperatures are low (average around $2.7^\circ$C), national HFMD cases are declining, and the region is in the post-Spring Festival period. The observed weekly count is 0 cases.

The proposed system predicts 0 cases with narrow intervals, achieving MAE 0. Context-free models, including ARIMA and a DeepSeek-based LLM baseline, tend to over-predict, with forecasts of 5 and 2 cases, respectively. Agent~1 outputs a slightly negative impact score (e.g., $I_t \approx -0.15$) and summarizes the situation as an off-season decline consistent with historical patterns and national reports. Retrieved guideline chunks describing typical winter troughs further support this interpretation. Agent~2 uses this information to keep the forecast near zero with low uncertainty, aligning well with the observed data.

These two case studies illustrate how the framework integrates seasonality, weather, school calendars, and guideline knowledge into a coherent explanation that directly influences the forecast trajectory.

\subsubsection{Failure case}

Despite overall strong performance, the system can fail when critical local signals are unobserved. For example, in the week of April~8, 2024, the Hangzhou series exhibits a sudden jump to 7 cases, likely driven by a localized outbreak (e.g., a single school cluster). However, external signals (weather, regional reports, school calendar) do not show marked changes at that time. Agent~1 produces a near-neutral impact score ($I_t \approx +0.1$) and a conservative narrative, and the final forecast remains close to the recent baseline, under-predicting the jump (MAE $\approx 6$).

This failure case highlights an inherent limitation: when relevant drivers are not reflected in the available external signals (e.g., hyper-local events), even a context-aware agent cannot anticipate abrupt changes. Incorporating additional data sources such as social media, syndromic surveillance, or school-level reports could help mitigate this limitation.

\subsection{Computational Efficiency}

Table~\ref{tab:efficiency} compares the inference time, approximate monetary cost per forecast, and interpretability across representative methods. As expected, LLM-based agents are slower and more expensive than classical and neural baselines, but offer substantially richer explanations.

In practical deployments, the latency of the two-agent framework can be mitigated using batch processing, caching of interpretations for similar contexts, or restricting the full agentic pipeline to high-risk periods, while relying on faster models during stable phases.

Overall, the results indicate that multi-agent LLM systems with structured context integration can match or exceed state-of-the-art time-series foundation models while providing interpretable, context-aware explanations that are valuable for public health decision-making.
